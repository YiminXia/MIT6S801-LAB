# Copy on write fork

[toc]

下一个是个非常常见的优化，很多操作系统都实现了这个优化。同时也是接下来你们要做的一个实验，这就是copy on write fork，也叫做COW fork。

当shell处理一个指令时，它会通过fork指令创建一个子进程。fork会拷贝shell进程，所以这个时候我们有了一个父进程（原来的shell进程）和一个子进程。此时子进程干的第一件事是调用exec运行一些其他的程序，比如运行echo。现在的情况是，fork创建了一个shell地址空间的完整的拷贝。而exec做的第一件事就是丢弃这个地址空间，取而代之是一个包含echo的地址空间。

这块看起来有点浪费。

![img](.assets/image%20(269).png)

所以，我们最开始有一个父进程的虚拟地址空间，然后我们有了个子进程的虚拟地址空间。在物理内存中，xv6中的shell通常会有4个page。当调用fork时候，基本上就创建了4个新的物理page，然后将父进程中的内容拷贝到子进程的4个新创建的物理页中。

![img](.assets/image%20(349).png)

当执行exec时候，我们会释放这些物理page中的内容，然后将echo的内容加载进来。所以对于这个特定的场景有一个很有效的优化：当我们创建子进程时，并不直接创建新的物理page，而是直接共享父进程的物理内存page。所以这里我们直接设置子进程的PTE指向父进程的物理page。

当然，再次要提及的是，我们这里需要非常小心。因为一旦子进程想要修改这些内存的内容，相应的修改对父进程是不可见的，因为我们希望在父进程与子进程之间有“强隔离性”。所以我们需要更加小心一些，我们可以将父进程与子进程的PTE都设置成只读的。

![img](.assets/image%20(313).png)

在某个时间点，当我们需要更改内存时，我们会得到一个page fault。因为父进程和子进程都会继续运行，而父进程或者子进程都可能会执行store指令来更新一些全局变量，这时就会触发page fault，因为现在在向一个只读的PTE写数据。

在得到page fault之后，我们先拷贝物理page，我们这里假设执行store指令的是子进程，那么我们会分配一个新的物理内存page，然后将page fault相关的物理内存page拷贝到新分配的物理内存page中，然后将新分配的物理page映射到子进程。这时，新分配的物理内存page只对子进程的地址空间可见，所以我们可以将相应的PTE设置为“可读写”。对于刚刚触发page fault的物理page，此时也只对父进程可见，相应的PTE也变成"可读写"。

![img](.assets/image%20(237).png)

所以现在我们拷贝了一个page，将新的page映射到相应的子进程的地址空间，并重新执行指令。重新执行指令指的是调用userret函数。（注，详见6.8），也就是lec06中介绍返回用户地址空间的机制。

![img](.assets/image%20(328).png)

>学生提问：我们如何发现父进程写了这部分内存地址？是与子进程相同的方法吗？
>
>Frans教授：是的，因为子进程的地址空间来自于父进程的地址空间的拷贝。如果我们使用了特定的虚拟地址，因为地址空间时相同的，不论是父进程还是子进程，都会有相同的处理方式。
>
>学生提问：对于一些没有父进程的进程，比如系统启动的第一个进程，它会对于自己的PTE设置成只读吗？还是设置成可读写的，然后再fork的时候在修改成只读的？
>
>Frans教授：这取决于你。实际上在Lazy lab之后，会有一个copy-on-write lab。在这个lab中，你自己可以选择实现方式。当然最简单的实现方式就是将PTE设置成只读的，当你要写这些page时，你会得到一个page fault，之后你可以再按照上面的流程进行处理。
>
>学生提问：因为我们经常会拷贝用户进程对应的page，内存硬件有没有实现特定的指令来完成拷贝，因为通常来说内存会有一些读写指令，但是因为我们现在有了从page a拷贝到page b的需求，会有相应的指令吗？
>
>Frans教授：x86有硬件的指令可以用来拷贝一段内存。但是RISC-V并没有这样的指令。当然在一个高性能的实现中，所有这些读写操作都会流水线化，并且按照内存带宽速度来运行。
>
>在我们的例子中，我们只需要拷贝一个page，对于一个未修改的xv6系统，我们需要拷贝4个page。所以这里的方法明显更好，因为内存消耗的更少，并且性能会更高，fork会执行的更快。
>
>学生提问：当发生page fault时，我们其实是在向一个只读地址执行写操作。内核该如何分辨现在是一个copy-on-write fork的场景，而不是应用程序在向一个正常的只读地址写数据。是不是说默认情况下，用户程序的PTE都是可读写的，除非在copy-on-write fork的场景下才可能出现只读PTE？
>
>Frans教授：内核必须要能够识别这是一个copy-on-write场景。几乎所有的page table硬件都支持这一点。我们之前并没有提到相关的内容，下图是一个常见的多级page table。对于PTE的标志位，我之前介绍过0bit到7bit，但是没有介绍最后两位的RSW。这两位保留给supervisor software使用，supervisor software指的是内核。内核可以随意使用这两位bit位。所以可以做的一件事就是，将bit8标识位当前是一个copy-on-write page。
>
>![img](.assets/image%20(225).png)
>
>当内核管理这些page table时，对于copy-on-write相关的page，内核可以设置相应的bit位，这样当发生page fault时，我们可以发现如果copy-on-write bit位设置了，我们可以执行相应的操作了。否则的话，比如说Lazy allocation，我们就做一些其他的处理操作。
>
>copy-on-write lab中，你们会使用RSW在PTE中设置一个copy-on-write标志位。

在copy-on-write lab中，还有个细节需要注意。目前在xv6中，除了trampoline page外，一个物理内存page只属于一个用户进程。trampoline page永远也不会释放，所以也不是什么大问题。但是对于这里的物理内存page，现在有多个用户进程或者说多个地址空间都指向相同的物理内存page，举个例子，当父进程退出我们需要更加小心，因为我们要判断是否能立即释放相应的物理page。如果有子进程还在使用这些物理内存page，而内核又释放了这些物理page，我们将会出问题。我们现在释放内存page的依据是什么呢？

我们需要对于每一个物理内存page的引用进行计数，当我们释放虚拟page时，我们将物理内存page的引用数减1，如果引用数等于0，那么我们就能释放物理内存page。所以在copy-on-write lab中，你们需要引入一些额外的数据结构或者元数据信息来完成引用计数。

>学生提问：我们应该在哪里存储这些计数呢？因为如果我们需要对每一个物理内存page的引用计数的话，这些计数可能会有很多。
>
>Frans教授：对于每个物理内存page，我们都需要做引用计数，也就是对于每个4096个字节，我们都需要维护一个引用计数（似乎没有回答问题）
>
>学生提问：我们可以将引用计数存储在RSW对应的2个bit中吗？并且限制不超过4个引用？
>
>Frans教授：讲道理，如果引用超过了4次，那么将会是一个问题。因为一个内存引用超过了4次，你将不能再使用这里的优化了。但是这里的实现方式是自由的。
>
>学生提问：真有必要额外增加一位来表示当前的page是copy-on-write吗？因为内核可以维护有关进程的一些信息
>
>Frans教授：是的，你可以在管理用户地址空间时维护一些其他的元数据信息，这样你就知道这部分虚拟内存地址如果发生了page fault，那么必然是copy-on-write场景。实际上，在后面的一个实验中，你们需要出于相同的原因扩展xv6管理的元数据。在你们完成这些实验时，具体的实现是很自由的。

